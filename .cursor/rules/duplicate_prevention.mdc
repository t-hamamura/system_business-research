# é‡è¤‡å®Ÿè¡Œé˜²æ­¢æ©Ÿèƒ½ãƒ«ãƒ¼ãƒ«ï¼ˆv2.0: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»åŠ¹ç‡åŒ–å¯¾å¿œï¼‰

## æ¦‚è¦
åŒä¸€ã¾ãŸã¯é¡ä¼¼ã®èª¿æŸ»ã®é‡è¤‡å®Ÿè¡Œã‚’é˜²æ­¢ã—ã€ãƒªã‚½ãƒ¼ã‚¹ã®ç„¡é§„é£ã„ã‚’é˜²ãã€‚ãƒãƒƒã‚·ãƒ¥å€¤ã«ã‚ˆã‚‹é‡è¤‡åˆ¤å®šã¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Ÿè£…ã€‚

## é‡è¤‡å®Ÿè¡Œé˜²æ­¢æ©Ÿèƒ½
```python
import hashlib
import json
import os
from datetime import datetime, timedelta
import difflib

def normalize_query(query):
    """
    ã‚¯ã‚¨ãƒªã®æ­£è¦åŒ–ï¼ˆé¡ä¼¼åˆ¤å®šç”¨ï¼‰
    """
    # å°æ–‡å­—åŒ–
    query = query.lower()
    
    # ä¸è¦ãªæ–‡å­—é™¤å»
    import re
    query = re.sub(r'[ï¼Ÿ?ï¼!ã€‚ã€,\s]+', ' ', query)
    
    # åŒç¾©èªçµ±ä¸€
    synonyms = {
        'å¸‚å ´èª¿æŸ»': 'ãƒãƒ¼ã‚±ãƒƒãƒˆèª¿æŸ»',
        'ç«¶åˆåˆ†æ': 'ç«¶åˆèª¿æŸ»',
        'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒœã‚¤ã‚¹': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼èª¿æŸ»',
        'ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»': 'ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ'
    }
    
    for original, normalized in synonyms.items():
        query = query.replace(original, normalized)
    
    return query.strip()

def generate_query_hash(research_type, target_query):
    """
    ã‚¯ã‚¨ãƒªã®ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
    """
    # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ä½œæˆ
    normalized = normalize_query(target_query)
    
    # èª¿æŸ»ã‚¿ã‚¤ãƒ—ã¨ã‚¯ã‚¨ãƒªã‚’çµåˆ
    combined = f"{research_type}:{normalized}"
    
    # ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
    return hashlib.md5(combined.encode()).hexdigest()

def check_duplicate_execution(research_type, query, cache_hours=24):
    """
    é‡è¤‡å®Ÿè¡Œã‚’ãƒã‚§ãƒƒã‚¯
    """
    # ã‚¯ã‚¨ãƒªã®ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
    query_hash = generate_query_hash(research_type, query)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    cache_dir = "Flow/.cache"
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f"cache_{query_hash}.json")
    
    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            cached_time = datetime.fromisoformat(cache_data["timestamp"])
            time_diff = datetime.now() - cached_time
            
            if time_diff < timedelta(hours=cache_hours):
                # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
                similarity = calculate_query_similarity(query, cache_data["original_query"])
                
                return {
                    "is_duplicate": True,
                    "cache_age_hours": time_diff.total_seconds() / 3600,
                    "similarity": similarity,
                    "message": f"âš ï¸ {cache_hours}æ™‚é–“ä»¥å†…ã«é¡ä¼¼ã®èª¿æŸ»ã‚’å®Ÿè¡Œæ¸ˆã¿ã§ã™",
                    "previous_result": cache_data.get("result_path"),
                    "executed_at": cache_data["timestamp"],
                    "original_query": cache_data["original_query"],
                    "cache_file": cache_file
                }
        
        except (json.JSONDecodeError, KeyError, ValueError):
            # ç ´æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å‰Šé™¤
            try:
                os.remove(cache_file)
            except:
                pass
    
    # é¡ä¼¼ã‚¯ã‚¨ãƒªã®æ¤œç´¢
    similar_cache = find_similar_cache(research_type, query, cache_hours)
    if similar_cache:
        return similar_cache
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨˜éŒ²é–¢æ•°ã‚’è¿”ã™
    def record_cache(result_path, execution_log=None):
        cache_data = {
            "query_hash": query_hash,
            "research_type": research_type,
            "original_query": query,
            "normalized_query": normalize_query(query),
            "timestamp": datetime.now().isoformat(),
            "result_path": result_path,
            "execution_log": execution_log,
            "cache_version": "2.0"
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Warning: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨˜éŒ²ã«å¤±æ•—: {e}")
    
    return {
        "is_duplicate": False,
        "record_function": record_cache,
        "query_hash": query_hash
    }

def calculate_query_similarity(query1, query2):
    """
    ã‚¯ã‚¨ãƒªã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
    """
    norm1 = normalize_query(query1)
    norm2 = normalize_query(query2)
    
    # å®Œå…¨ä¸€è‡´
    if norm1 == norm2:
        return 1.0
    
    # æ–‡å­—åˆ—é¡ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡åº¦
    words1 = set(norm1.split())
    words2 = set(norm2.split())
    
    if words1 and words2:
        keyword_overlap = len(words1 & words2) / len(words1 | words2)
        # æ–‡å­—åˆ—é¡ä¼¼åº¦ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡åº¦ã®åŠ é‡å¹³å‡
        similarity = 0.6 * similarity + 0.4 * keyword_overlap
    
    return similarity

def find_similar_cache(research_type, query, cache_hours=24, similarity_threshold=0.8):
    """
    é¡ä¼¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢
    """
    cache_dir = "Flow/.cache"
    if not os.path.exists(cache_dir):
        return None
    
    cutoff_time = datetime.now() - timedelta(hours=cache_hours)
    best_match = None
    best_similarity = 0
    
    try:
        for filename in os.listdir(cache_dir):
            if not filename.startswith("cache_") or not filename.endswith(".json"):
                continue
            
            cache_file = os.path.join(cache_dir, filename)
            
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # èª¿æŸ»ã‚¿ã‚¤ãƒ—ãƒã‚§ãƒƒã‚¯
                if cache_data.get("research_type") != research_type:
                    continue
                
                # æ™‚é–“ãƒã‚§ãƒƒã‚¯
                cached_time = datetime.fromisoformat(cache_data["timestamp"])
                if cached_time < cutoff_time:
                    continue
                
                # é¡ä¼¼åº¦è¨ˆç®—
                similarity = calculate_query_similarity(query, cache_data["original_query"])
                
                if similarity > best_similarity and similarity >= similarity_threshold:
                    best_similarity = similarity
                    best_match = {
                        "is_duplicate": True,
                        "cache_age_hours": (datetime.now() - cached_time).total_seconds() / 3600,
                        "similarity": similarity,
                        "message": f"âš ï¸ é¡ä¼¼ã®èª¿æŸ»ãŒ{cache_hours}æ™‚é–“ä»¥å†…ã«å®Ÿè¡Œæ¸ˆã¿ã§ã™ï¼ˆé¡ä¼¼åº¦: {similarity:.1%}ï¼‰",
                        "previous_result": cache_data.get("result_path"),
                        "executed_at": cache_data["timestamp"],
                        "original_query": cache_data["original_query"],
                        "cache_file": cache_file
                    }
            
            except (json.JSONDecodeError, KeyError, ValueError):
                continue
    
    except OSError:
        pass
    
    return best_match

def cleanup_old_cache(max_age_days=7):
    """
    å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    """
    cache_dir = "Flow/.cache"
    if not os.path.exists(cache_dir):
        return {"cleaned": 0, "errors": 0}
    
    cutoff_time = datetime.now() - timedelta(days=max_age_days)
    cleaned_count = 0
    error_count = 0
    
    try:
        for filename in os.listdir(cache_dir):
            if not filename.startswith("cache_") or not filename.endswith(".json"):
                continue
            
            cache_file = os.path.join(cache_dir, filename)
            
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯
                file_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
                
                if file_time < cutoff_time:
                    os.remove(cache_file)
                    cleaned_count += 1
            
            except Exception:
                error_count += 1
    
    except OSError:
        error_count += 1
    
    return {"cleaned": cleaned_count, "errors": error_count}

def get_cache_statistics():
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã®å–å¾—
    """
    cache_dir = "Flow/.cache"
    if not os.path.exists(cache_dir):
        return {"total_cache": 0, "valid_cache": 0, "total_size_kb": 0}
    
    total_files = 0
    valid_files = 0
    total_size = 0
    research_types = {}
    
    try:
        for filename in os.listdir(cache_dir):
            if not filename.startswith("cache_") or not filename.endswith(".json"):
                continue
            
            cache_file = os.path.join(cache_dir, filename)
            
            try:
                total_size += os.path.getsize(cache_file)
                total_files += 1
                
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                if "research_type" in cache_data and "timestamp" in cache_data:
                    valid_files += 1
                    research_type = cache_data["research_type"]
                    research_types[research_type] = research_types.get(research_type, 0) + 1
            
            except:
                continue
    
    except OSError:
        pass
    
    return {
        "total_cache": total_files,
        "valid_cache": valid_files,
        "total_size_kb": round(total_size / 1024, 1),
        "research_types": research_types
    }

def force_clear_cache():
    """
    å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å¼·åˆ¶å‰Šé™¤
    """
    cache_dir = "Flow/.cache"
    if not os.path.exists(cache_dir):
        return {"status": "success", "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“"}
    
    try:
        import shutil
        shutil.rmtree(cache_dir)
        os.makedirs(cache_dir, exist_ok=True)
        
        return {
            "status": "success", 
            "message": "å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
        }
    
    except Exception as e:
        return {
            "status": "error",
            "message": f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }
```

## ä½¿ç”¨æ–¹æ³•

### 1. èª¿æŸ»é–‹å§‹å‰ã®ãƒã‚§ãƒƒã‚¯
```python
# é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
duplicate_check = check_duplicate_execution("market_research", "é›»å‹•æ­¯ãƒ–ãƒ©ã‚·å¸‚å ´èª¿æŸ»")

if duplicate_check["is_duplicate"]:
    print(duplicate_check["message"])
    print(f"å‰å›å®Ÿè¡Œ: {duplicate_check['executed_at']}")
    print(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«: {duplicate_check['previous_result']}")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¶™ç¶šç¢ºèª
    user_input = input("ãã‚Œã§ã‚‚æ–°ã—ãèª¿æŸ»ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
    if user_input.lower() != 'y':
        exit()
```

### 2. èª¿æŸ»å®Œäº†å¾Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨˜éŒ²
```python
# èª¿æŸ»å®Œäº†å¾Œ
if not duplicate_check["is_duplicate"]:
    duplicate_check["record_function"](
        result_path="Stock/project/market-research/market_research_20250127_143025.md",
        execution_log=execution_log_data
    )
```

### 3. å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
```python
# é€±1å›ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
cleanup_result = cleanup_old_cache(max_age_days=7)
print(f"å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ {cleanup_result['cleaned']} ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
```

## ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã‚³ãƒãƒ³ãƒ‰

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã®ç¢ºèª
```
ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ³ã‚’ç¢ºèªã—ã¦ã€
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ‰‹å‹•å‰Šé™¤
```
ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã€
ã€Œå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¦ã€
```

### é‡è¤‡ãƒã‚§ãƒƒã‚¯ã®ç„¡åŠ¹åŒ–
```
ã€Œé‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’ç„¡è¦–ã—ã¦å®Ÿè¡Œã—ã¦ã€
ã€Œå¼·åˆ¶çš„ã«æ–°ã—ãèª¿æŸ»ã—ã¦ã€
```

## é‡è¤‡æ¤œå‡ºæ™‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

```
âš ï¸ é¡ä¼¼ã®èª¿æŸ»ãŒ24æ™‚é–“ä»¥å†…ã«å®Ÿè¡Œæ¸ˆã¿ã§ã™

ğŸ“‹ å‰å›ã®èª¿æŸ»æƒ…å ±:
- å®Ÿè¡Œæ—¥æ™‚: 2025-01-27 12:30:25
- èª¿æŸ»ã‚¯ã‚¨ãƒª: ã€Œé›»å‹•æ­¯ãƒ–ãƒ©ã‚·å¸‚å ´åˆ†æã€
- é¡ä¼¼åº¦: 85%
- çµæœãƒ•ã‚¡ã‚¤ãƒ«: Stock/dental-products/market-research/market_research_20250127_123025.md

ğŸ”„ é¸æŠè‚¢:
1. å‰å›ã®çµæœã‚’å†åˆ©ç”¨ã™ã‚‹
2. ãã‚Œã§ã‚‚æ–°ã—ãèª¿æŸ»ã‚’å®Ÿè¡Œã™ã‚‹
3. å‰å›ã®çµæœã‚’ç¢ºèªã—ã¦ã‹ã‚‰åˆ¤æ–­ã™ã‚‹

ã©ã¡ã‚‰ã‚’å¸Œæœ›ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼ˆ1/2/3ï¼‰
```

## è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

config/research_config.yamlã«è¿½åŠ ï¼š
```yaml
duplicate_prevention:
  enabled: true
  cache_hours: 24              # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ™‚é–“
  similarity_threshold: 0.8    # é¡ä¼¼åˆ¤å®šé–¾å€¤
  auto_cleanup_days: 7         # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æœŸé–“
  max_cache_size_mb: 100       # æœ€å¤§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
  ignore_user_patterns:        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç„¡è¦–ãƒ‘ã‚¿ãƒ¼ãƒ³
    - "å¼·åˆ¶"
    - "æ–°ã—ã"
    - "å†å®Ÿè¡Œ"
```